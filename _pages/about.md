---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Indranil Halder currently holds an associate researcher position at Harvard University. Earlier, he was the Quantum Initiative Fellow at the Center for the Fundamental Laws of Nature. He is also affiliated with the Center for Quantum Mathematics and Physics at the University of California, Davis. 


## Research interest: Theoretical high energy physics

During his early days at Harvard, Indranil Halder established a new [strong-weak duality](https://link.springer.com/article/10.1007/JHEP07(2023)049) closely related to ER=EPR - the fascinating connection between entanglement and geometry. Using the duality he has made distinct progress on the long-standing issue of [thermal microstate counting](https://link.springer.com/article/10.1007/JHEP05(2024)136) of blackholes. He has also proposed a framework to evaluate the [supersymmetric index in disordered systems](https://arxiv.org/abs/2504.05379) in terms of bi-local fields closely related to wormhole-like physics.

In the past, he worked extensively on theoretical developments of topological quantum computation, more precisely on Chern-Simons gauge theory coupled to matter. He discovered a [condensed phase](https://link.springer.com/article/10.1007/JHEP11(2018)177) and pointed out an exact [non-commutative structure](https://link.springer.com/article/10.1007/JHEP11(2019)089) in the presence of a background magnetic field. 


## Research interest: Mathematical aspects of machine learning 

He has defined and studied an analytically tractable one-step diffusion model. In the context of higher-dimensional statistics, using the method of deterministic equivalence, he has proved a theorem that presents an explicit formula for the Kullback-Leibler divergence between the generated and sampling distribution, taken to be isotropic Gaussian, showing the effect of finite diffusion time and noise scale. It shows that the monotonic fall phase of Kullback-Leibler divergence begins when the training dataset size reaches the dimension of the data points. Finally, for large-scale practical diffusion models, it explains why a higher number of diffusion steps enhances production quality. 
