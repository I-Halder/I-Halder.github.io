---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Indranil Halder currently holds an associate researcher position at Harvard University. Earlier, he was the Quantum Initiative Fellow at the Center for the Fundamental Laws of Nature. He is also affiliated with the Center for Quantum Mathematics and Physics at the University of California, Davis. 

# Research Interest

## Theoretical High Energy Physics

During his early days at Harvard, Indranil Halder established a new [strong-weak duality](https://link.springer.com/article/10.1007/JHEP07(2023)049) closely related to ER=EPR - the fascinating connection between entanglement and geometry. Using the duality he has made distinct progress on the long-standing issue of [thermal microstate counting](https://link.springer.com/article/10.1007/JHEP05(2024)136) in gravitational systems.

In the past, he worked extensively on theoretical developments of topological quantum computation, more precisely on Chern-Simons gauge theory coupled to matter. He discovered a [condensed phase](https://link.springer.com/article/10.1007/JHEP11(2018)177) and pointed out an exact [non-commutative structure](https://link.springer.com/article/10.1007/JHEP11(2019)089) in these theoriesin the presence of a background magnetic field. 


## Theoretical Machine Learning

Indranil Halder has been fascinated by the developments in the domain of generative artificial intelligence. Recently, he proposed a [simplified diffusion model](https://arxiv.org/pdf/2411.17807) that captures the transition from memorization to generalization. Within this framework, he formulated a theorem addressing the scaling law of the Kullback–Leibler divergence between the generated probability distribution and the underlying distribution on which the model is trained. Furthermore, he demonstrated that certain outcomes predicted by this model are empirically validated in large-scale diffusion models commonly observed in practice. He is also actively engaged in projects focused on [knowledge distillation](https://github.com/I-Halder/knowledge-distillation-of-large-language-models) and the [inference-time characteristics](https://github.com/I-Halder/statistical-inference-with-large-language-models) of large language models.
